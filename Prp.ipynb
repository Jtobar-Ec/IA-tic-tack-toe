{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6LdvbFT_iA6d",
        "outputId": "ab9ee793-a0d6-429a-96cf-24749b2fc7f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Skipping datasets as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting datasets\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "# Paso 1: Instalar bibliotecas necesarias\n",
        "!pip uninstall datasets -y\n",
        "!pip install datasets --upgrade\n",
        "!pip install transformers datasets torch huggingface-hub accelerate --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7etKHD2EMMm"
      },
      "outputs": [],
      "source": [
        "# Paso 2: Importar las bibliotecas\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Paso 3: Configurar Hugging Face Access Token\n",
        "from huggingface_hub import login\n",
        "login(\"tu_token_aqui\")\n",
        "\n",
        "\n",
        "\n",
        "# Juez (GPT-2)\n",
        "print(\"Cargando modelo del juez (GPT-2)...\")\n",
        "judge_model_name = \"openai-community/gpt2\"\n",
        "judge_model = AutoModelForCausalLM.from_pretrained(judge_model_name)\n",
        "judge_tokenizer = AutoTokenizer.from_pretrained(judge_model_name)\n",
        "\n",
        "# Jugador 1 (Pythia 160M)\n",
        "print(\"Cargando modelo del Jugador 1 (Pythia 160M)...\")\n",
        "player1_model_name = \"EleutherAI/pythia-160m\"\n",
        "player1_model = AutoModelForCausalLM.from_pretrained(player1_model_name)\n",
        "player1_tokenizer = AutoTokenizer.from_pretrained(player1_model_name)\n",
        "\n",
        "# Jugador 2 (Cerebras-GPT 111M)\n",
        "print(\"Cargando modelo del Jugador 2 (Cerebras-GPT 111M)...\")\n",
        "player2_model_name = \"cerebras/Cerebras-GPT-111M\"\n",
        "player2_model = AutoModelForCausalLM.from_pretrained(player2_model_name)\n",
        "player2_tokenizer = AutoTokenizer.from_pretrained(player2_model_name)\n",
        "\n",
        "# Paso 5: Configurar dispositivo y mover modelos\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "judge_model.to(device)\n",
        "player1_model.to(device)\n",
        "player2_model.to(device)\n",
        "\n",
        "# Paso 6: Confirmar configuración\n",
        "print(\"\\nModelos cargados exitosamente:\")\n",
        "print(f\"- Juez: {judge_model_name}\")\n",
        "print(f\"- Jugador 1: {player1_model_name}\")\n",
        "print(f\"- Jugador 2: {player2_model_name}\")\n",
        "print(\"Modelos movidos al dispositivo:\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEXiiwY8ZYJK",
        "outputId": "5c708db6-9976-49f6-f759-d6068682136f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ejemplos generados y guardados en 'tictactoe_examples.json'\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import json\n",
        "\n",
        "def generate_board():\n",
        "    \"\"\"Genera un tablero aleatorio de tres en raya.\"\"\"\n",
        "    symbols = [\"X\", \"O\", \" \"]\n",
        "    board = [[random.choice(symbols) for _ in range(3)] for _ in range(3)]\n",
        "    return board\n",
        "\n",
        "def check_winner(board):\n",
        "    \"\"\"Verifica si hay un ganador en el tablero.\"\"\"\n",
        "    for row in board:\n",
        "        if row[0] == row[1] == row[2] and row[0] != \" \":\n",
        "            return row[0]\n",
        "    for col in range(3):\n",
        "        if board[0][col] == board[1][col] == board[2][col] and board[0][col] != \" \":\n",
        "            return board[0][col]\n",
        "    if board[0][0] == board[1][1] == board[2][2] and board[0][0] != \" \":\n",
        "        return board[0][0]\n",
        "    if board[0][2] == board[1][1] == board[2][0] and board[0][2] != \" \":\n",
        "        return board[0][2]\n",
        "    return None\n",
        "\n",
        "def is_full(board):\n",
        "    \"\"\"Verifica si el tablero está lleno.\"\"\"\n",
        "    return all(cell != \" \" for row in board for cell in row)\n",
        "\n",
        "def generate_examples(n=10):\n",
        "    \"\"\"Genera ejemplos de jugadas válidas, inválidas y resultados del juego.\"\"\"\n",
        "    examples = []\n",
        "    for _ in range(n):\n",
        "        board = generate_board()\n",
        "        winner = check_winner(board)\n",
        "        if winner:\n",
        "            output = f\"Estado válido. Jugador {winner} gana.\"\n",
        "        elif is_full(board):\n",
        "            output = \"Estado válido. El juego termina en empate.\"\n",
        "        else:\n",
        "            output = \"Estado válido. No hay ganador aún.\"\n",
        "\n",
        "        input_text = f\"Tablero actual:\\n\" + \"\\n\".join([\" | \".join(row) for row in board])\n",
        "        examples.append({\"input\": input_text, \"output\": output})\n",
        "\n",
        "    return examples\n",
        "\n",
        "# Generar ejemplos y guardarlos en JSON\n",
        "examples = generate_examples(100)\n",
        "with open(\"tictactoe_examples.json\", \"w\") as f:\n",
        "    json.dump(examples, f, indent=4)\n",
        "\n",
        "print(\"Ejemplos generados y guardados en 'tictactoe_examples.json'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6722Xk8GeAtc"
      },
      "outputs": [],
      "source": [
        "# Importar las bibliotecas necesarias\n",
        "from datasets import load_dataset\n",
        "from transformers import Trainer, TrainingArguments, AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Cargar el tokenizador y el modelo preentrenado\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
        "judge_model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
        "\n",
        "# Asignar el eos_token como pad_token y configurar el padding\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\"\n",
        "\n",
        "# Cargar los datos de entrenamiento y evaluación desde los archivos JSON\n",
        "train_dataset = load_dataset(\"json\", data_files=\"tictactoe_examples.json\")\n",
        "eval_dataset = load_dataset(\"json\", data_files=\"tictactoe_eval.json\")\n",
        "\n",
        "# Definir la función de preprocesamiento para tokenizar las entradas y salidas\n",
        "def preprocess(examples):\n",
        "    inputs = examples[\"input\"]\n",
        "    outputs = examples[\"output\"]\n",
        "    model_inputs = tokenizer(inputs, truncation=True, padding=\"max_length\", max_length=128)\n",
        "    labels = tokenizer(outputs, truncation=True, padding=\"max_length\", max_length=128)[\"input_ids\"]\n",
        "    model_inputs[\"labels\"] = labels\n",
        "    return model_inputs\n",
        "\n",
        "# Aplicar el preprocesamiento a los conjuntos de datos\n",
        "tokenized_train_dataset = train_dataset[\"train\"].map(preprocess, batched=True)\n",
        "tokenized_eval_dataset = eval_dataset[\"train\"].map(preprocess, batched=True)\n",
        "\n",
        "# Configurar los argumentos de entrenamiento\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./tictactoe_judge\",\n",
        "    evaluation_strategy=\"steps\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        "    save_steps=500,\n",
        "    logging_steps=10,\n",
        "    eval_steps=100  # Frecuencia de evaluación durante el entrenamiento\n",
        ")\n",
        "\n",
        "# Crear el entrenador\n",
        "trainer = Trainer(\n",
        "    model=judge_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_eval_dataset,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# Iniciar el entrenamiento\n",
        "trainer.train()\n",
        "\n",
        "# Guardar el modelo entrenado\n",
        "trainer.save_model(\"./tictactoe_judge_model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0jNdb02GsCU",
        "outputId": "8c0b84a1-57fc-4f2c-900b-54377b16dc70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resultados de 100 partidas guardados en 'evaluated_jugadas_100.json'\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Evaluación de Jugadas con el Juez\n",
        "\n",
        "\"\"\"\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import random\n",
        "import json\n",
        "\n",
        "# Cargar el modelo y el tokenizador del juez entrenado\n",
        "judge_model = AutoModelForCausalLM.from_pretrained(\"./tictactoe_judge_model\")\n",
        "judge_tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
        "\n",
        "# Configurar el token de padding\n",
        "judge_tokenizer.pad_token = judge_tokenizer.eos_token\n",
        "judge_tokenizer.padding_side = \"left\"\n",
        "\n",
        "# Función para inicializar un tablero vacío\n",
        "def inicializar_tablero():\n",
        "    return [[\" \" for _ in range(3)] for _ in range(3)]\n",
        "\n",
        "# Función para representar el tablero como texto\n",
        "def representar_tablero(tablero):\n",
        "    return \"\\n\".join([\" | \".join(fila) for fila in tablero])\n",
        "\n",
        "# Función para verificar si hay un ganador\n",
        "def verificar_ganador(tablero):\n",
        "    for fila in tablero:\n",
        "        if fila[0] == fila[1] == fila[2] and fila[0] != \" \":\n",
        "            return fila[0]\n",
        "\n",
        "    for col in range(3):\n",
        "        if tablero[0][col] == tablero[1][col] == tablero[2][col] and tablero[0][col] != \" \":\n",
        "            return tablero[0][col]\n",
        "\n",
        "    if tablero[0][0] == tablero[1][1] == tablero[2][2] and tablero[0][0] != \" \":\n",
        "        return tablero[0][0]\n",
        "\n",
        "    if tablero[0][2] == tablero[1][1] == tablero[2][0] and tablero[0][2] != \" \":\n",
        "        return tablero[0][2]\n",
        "\n",
        "    return None\n",
        "\n",
        "# Función para verificar si el tablero está lleno\n",
        "def tablero_lleno(tablero):\n",
        "    return all(cell != \" \" for fila in tablero for cell in fila)\n",
        "\n",
        "# Función para jugar una partida completa\n",
        "def jugar_partida():\n",
        "    tablero = inicializar_tablero()\n",
        "    turno = random.choice([\"X\", \"O\"])\n",
        "    partida = []\n",
        "\n",
        "    while True:\n",
        "        movimiento = random.randint(1, 9)\n",
        "        fila, col = divmod(movimiento - 1, 3)\n",
        "\n",
        "        if tablero[fila][col] == \" \":\n",
        "            tablero[fila][col] = turno\n",
        "            entrada = f\"Tablero:\\n{representar_tablero(tablero)}\\nTurno: {turno}\\nMovimiento: {movimiento}\"\n",
        "            salida = \"\"\n",
        "\n",
        "            # Verificar estado del juego\n",
        "            ganador = verificar_ganador(tablero)\n",
        "            if ganador:\n",
        "                salida = f\"Jugada válida. Resultado: Victoria de {ganador}.\"\n",
        "                partida.append({\"input\": entrada, \"expected_output\": salida})\n",
        "                break\n",
        "            elif tablero_lleno(tablero):\n",
        "                salida = \"Jugada válida. Resultado: Empate.\"\n",
        "                partida.append({\"input\": entrada, \"expected_output\": salida})\n",
        "                break\n",
        "            else:\n",
        "                salida = \"Jugada válida. No hay ganador aún.\"\n",
        "\n",
        "            partida.append({\"input\": entrada, \"expected_output\": salida})\n",
        "\n",
        "            # Cambiar turno\n",
        "            turno = \"O\" if turno == \"X\" else \"X\"\n",
        "\n",
        "    return partida\n",
        "\n",
        "# Generar 100 partidas completas\n",
        "def generar_partidas(n=100):\n",
        "    todas_las_partidas = []\n",
        "    for _ in range(n):\n",
        "        partida = jugar_partida()\n",
        "        todas_las_partidas.extend(partida)\n",
        "    return todas_las_partidas\n",
        "\n",
        "# Evaluar las jugadas con el juez\n",
        "def evaluar_con_juez(partidas):\n",
        "    resultados = []\n",
        "    for jugada in partidas:\n",
        "        input_text = jugada[\"input\"]\n",
        "\n",
        "        # Tokenizar la entrada\n",
        "        inputs_tokenized = judge_tokenizer(\n",
        "            input_text, truncation=True, padding=\"max_length\", max_length=128, return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Generar salida del juez\n",
        "        outputs = judge_model.generate(\n",
        "            **inputs_tokenized,\n",
        "            max_new_tokens=50,\n",
        "            pad_token_id=judge_tokenizer.pad_token_id\n",
        "        )\n",
        "        generated_output = judge_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # Guardar el resultado\n",
        "        resultados.append({\n",
        "            \"input\": input_text,\n",
        "            \"expected_output\": jugada[\"expected_output\"],\n",
        "            \"generated_output\": generated_output,\n",
        "            \"correct\": jugada[\"expected_output\"] == generated_output\n",
        "        })\n",
        "    return resultados\n",
        "\n",
        "# Generar y evaluar 100 partidas\n",
        "partidas = generar_partidas(100)\n",
        "resultados = evaluar_con_juez(partidas)\n",
        "\n",
        "\n",
        "# Guardar los resultados en un archivo JSON\n",
        "with open(\"evaluated_jugadas_100.json\", \"w\") as f:\n",
        "    json.dump(resultados, f, indent=4)\n",
        "    print(\"Resultados de 100 partidas guardados en 'evaluated_jugadas_100.json'\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "RyCUniCHK0iB",
        "outputId": "a0529c7d-28dd-4f52-818f-c5673914a357"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-6-ef213f9f3908>:71: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer_player1 = Trainer(\n",
            "max_steps is given, it will override any value given in num_train_epochs\n",
            "<ipython-input-6-ef213f9f3908>:78: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer_player2 = Trainer(\n",
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entrenando Jugador 1...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [200/200 01:41, Epoch 8/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.342400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.053600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.037900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.024200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Jugador 1 entrenado y guardado.\n",
            "Entrenando Jugador 2...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [200/200 01:49, Epoch 8/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.517100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.105200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.087500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.048900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Jugador 2 entrenado y guardado.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
        "import json\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# Cargar los modelos y tokenizadores de los jugadores\n",
        "player1_model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-160m\")\n",
        "player1_tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-160m\")\n",
        "player1_tokenizer.pad_token = player1_tokenizer.eos_token\n",
        "player1_tokenizer.padding_side = \"left\"\n",
        "\n",
        "player2_model = AutoModelForCausalLM.from_pretrained(\"cerebras/Cerebras-GPT-111M\")\n",
        "player2_tokenizer = AutoTokenizer.from_pretrained(\"cerebras/Cerebras-GPT-111M\")\n",
        "player2_tokenizer.pad_token = player2_tokenizer.eos_token\n",
        "player2_tokenizer.padding_side = \"left\"\n",
        "\n",
        "# Cargar las jugadas evaluadas\n",
        "with open(\"evaluated_jugadas_100.json\", \"r\") as f:\n",
        "    evaluated_data = json.load(f)\n",
        "\n",
        "# Seleccionar una muestra representativa\n",
        "max_jugadas = 50  # Limitar a 50 jugadas para cada jugador\n",
        "jugadas_muestra = evaluated_data[:max_jugadas]\n",
        "\n",
        "# Crear un dataset personalizado para Hugging Face\n",
        "class TictactoeDataset(Dataset):\n",
        "    def __init__(self, jugadas, tokenizer):\n",
        "        self.examples = []\n",
        "        for jugada in jugadas:\n",
        "            inputs = tokenizer(\n",
        "                jugada[\"input\"], truncation=True, padding=\"max_length\", max_length=128, return_tensors=\"pt\"\n",
        "            )\n",
        "            labels = tokenizer(\n",
        "                jugada[\"expected_output\"], truncation=True, padding=\"max_length\", max_length=128, return_tensors=\"pt\"\n",
        "            )[\"input_ids\"]\n",
        "            inputs[\"labels\"] = labels\n",
        "            self.examples.append(inputs)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: val.squeeze(0) for key, val in self.examples[idx].items()}\n",
        "\n",
        "# Crear datasets para los jugadores\n",
        "player1_dataset = TictactoeDataset(jugadas_muestra, player1_tokenizer)\n",
        "player2_dataset = TictactoeDataset(jugadas_muestra, player2_tokenizer)\n",
        "\n",
        "# Configurar argumentos de entrenamiento\n",
        "training_args_player1 = TrainingArguments(\n",
        "    output_dir=\"./trained_player1_model\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=2,  # Tamaño de lote reducido\n",
        "    max_steps=200,  # Limitar a 200 pasos de entrenamiento\n",
        "    save_steps=100,  # Guardar el modelo cada 100 pasos\n",
        "    logging_steps=50,  # Registrar información cada 50 pasos\n",
        "    evaluation_strategy=\"no\"  # Sin evaluación durante el entrenamiento\n",
        ")\n",
        "\n",
        "training_args_player2 = TrainingArguments(\n",
        "    output_dir=\"./trained_player2_model\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=2,  # Tamaño de lote reducido\n",
        "    max_steps=200,  # Limitar a 200 pasos de entrenamiento\n",
        "    save_steps=100,  # Guardar el modelo cada 100 pasos\n",
        "    logging_steps=50,  # Registrar información cada 50 pasos\n",
        "    evaluation_strategy=\"no\"  # Sin evaluación durante el entrenamiento\n",
        ")\n",
        "\n",
        "# Crear entrenadores para los jugadores\n",
        "trainer_player1 = Trainer(\n",
        "    model=player1_model,\n",
        "    args=training_args_player1,\n",
        "    train_dataset=player1_dataset,\n",
        "    tokenizer=player1_tokenizer,\n",
        ")\n",
        "\n",
        "trainer_player2 = Trainer(\n",
        "    model=player2_model,\n",
        "    args=training_args_player2,\n",
        "    train_dataset=player2_dataset,\n",
        "    tokenizer=player2_tokenizer,\n",
        ")\n",
        "\n",
        "# Entrenar a Jugador 1\n",
        "print(\"Entrenando Jugador 1...\")\n",
        "trainer_player1.train()\n",
        "trainer_player1.save_model(\"./trained_player1_model\")\n",
        "print(\"Jugador 1 entrenado y guardado.\")\n",
        "\n",
        "# Entrenar a Jugador 2\n",
        "print(\"Entrenando Jugador 2...\")\n",
        "trainer_player2.train()\n",
        "trainer_player2.save_model(\"./trained_player2_model\")\n",
        "print(\"Jugador 2 entrenado y guardado.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lje-hRuGm2wQ",
        "outputId": "bfae381e-2500-4397-8661-959109832a3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " MODELO Pythia 160M (Jugador 1, X) hizo el movimiento 9\n",
            "[' ', ' ', ' ']\n",
            "[' ', ' ', ' ']\n",
            "[' ', ' ', 'X']\n",
            "\n",
            " MODELO Cerebras-GPT 111M (Jugador 2, O) hizo el movimiento 7\n",
            "[' ', ' ', ' ']\n",
            "[' ', ' ', ' ']\n",
            "['O', ' ', 'X']\n",
            "\n",
            " MODELO Pythia 160M (Jugador 1, X) hizo el movimiento 2\n",
            "[' ', 'X', ' ']\n",
            "[' ', ' ', ' ']\n",
            "['O', ' ', 'X']\n",
            "\n",
            " MODELO Cerebras-GPT 111M (Jugador 2, O) hizo el movimiento 5\n",
            "[' ', 'X', ' ']\n",
            "[' ', 'O', ' ']\n",
            "['O', ' ', 'X']\n",
            "\n",
            " MODELO Pythia 160M (Jugador 1, X) hizo el movimiento 8\n",
            "[' ', 'X', ' ']\n",
            "[' ', 'O', ' ']\n",
            "['O', 'X', 'X']\n",
            "\n",
            " MODELO Cerebras-GPT 111M (Jugador 2, O) hizo el movimiento 4\n",
            "[' ', 'X', ' ']\n",
            "['O', 'O', ' ']\n",
            "['O', 'X', 'X']\n",
            "\n",
            " MODELO Pythia 160M (Jugador 1, X) hizo el movimiento 1\n",
            "['X', 'X', ' ']\n",
            "['O', 'O', ' ']\n",
            "['O', 'X', 'X']\n",
            "\n",
            " MODELO Cerebras-GPT 111M (Jugador 2, O) hizo el movimiento 3\n",
            "['X', 'X', 'O']\n",
            "['O', 'O', ' ']\n",
            "['O', 'X', 'X']\n",
            "\n",
            "¡Victoria de MODELO Cerebras-GPT 111M (O)!\n",
            "Resultado final: O\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJZMLwW0rAZz",
        "outputId": "313da3ba-3b1b-49f4-ec03-31d58f993678"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMf-ChkIyMzc",
        "outputId": "1a4891d9-db36-432f-a077-aa4db19ab577"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "MODELO Pythia 160M (Jugador 1, X) hizo el movimiento 7\n",
            "[' ', ' ', ' ']\n",
            "[' ', ' ', ' ']\n",
            "['X', ' ', ' ']\n",
            "COMENTARIO JUEZ: El movimiento 7 por el jugador 'X' es válido.\n",
            "\n",
            "MODELO Cerebras-GPT 111M (Jugador 2, O) hizo el movimiento 1\n",
            "['O', ' ', ' ']\n",
            "[' ', ' ', ' ']\n",
            "['X', ' ', ' ']\n",
            "COMENTARIO JUEZ: El movimiento 1 por el jugador 'O' es válido.\n",
            "\n",
            "MODELO Pythia 160M (Jugador 1, X) hizo el movimiento 2\n",
            "['O', 'X', ' ']\n",
            "[' ', ' ', ' ']\n",
            "['X', ' ', ' ']\n",
            "COMENTARIO JUEZ: El movimiento 2 por el jugador 'X' es válido.\n",
            "\n",
            "MODELO Cerebras-GPT 111M (Jugador 2, O) hizo el movimiento 3\n",
            "['O', 'X', 'O']\n",
            "[' ', ' ', ' ']\n",
            "['X', ' ', ' ']\n",
            "COMENTARIO JUEZ: El movimiento 3 por el jugador 'O' es válido.\n",
            "\n",
            "MODELO Pythia 160M (Jugador 1, X) hizo el movimiento 5\n",
            "['O', 'X', 'O']\n",
            "[' ', 'X', ' ']\n",
            "['X', ' ', ' ']\n",
            "COMENTARIO JUEZ: El movimiento 5 por el jugador 'X' es válido.\n",
            "\n",
            "MODELO Cerebras-GPT 111M (Jugador 2, O) hizo el movimiento 6\n",
            "['O', 'X', 'O']\n",
            "[' ', 'X', 'O']\n",
            "['X', ' ', ' ']\n",
            "COMENTARIO JUEZ: El movimiento 6 por el jugador 'O' es válido.\n",
            "\n",
            "MODELO Pythia 160M (Jugador 1, X) hizo el movimiento 9\n",
            "['O', 'X', 'O']\n",
            "[' ', 'X', 'O']\n",
            "['X', ' ', 'X']\n",
            "COMENTARIO JUEZ: El movimiento 9 por el jugador 'X' es válido.\n",
            "\n",
            "MODELO Cerebras-GPT 111M (Jugador 2, O) hizo el movimiento 4\n",
            "['O', 'X', 'O']\n",
            "['O', 'X', 'O']\n",
            "['X', ' ', 'X']\n",
            "COMENTARIO JUEZ: El movimiento 4 por el jugador 'O' es válido.\n",
            "\n",
            "MODELO Pythia 160M (Jugador 1, X) hizo el movimiento 8\n",
            "['O', 'X', 'O']\n",
            "['O', 'X', 'O']\n",
            "['X', 'X', 'X']\n",
            "COMENTARIO JUEZ: El movimiento 8 por el jugador 'X' es válido.\n",
            "\n",
            "¡Victoria de MODELO Pythia 160M (X)!\n",
            "COMENTARIO JUEZ: ¡El jugador 'X' con el modelo Pythia 160M ganó!\n",
            "Posiciones ganadoras:\n",
            "O | *X* | O\n",
            "O | *X* | O\n",
            "X | *X* | X\n",
            "Resultado final: X\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Cargar los modelos y tokenizadores de los jugadores\n",
        "player1_model = AutoModelForCausalLM.from_pretrained(\"./trained_player1_model\")\n",
        "player1_tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-160m\")\n",
        "player1_tokenizer.pad_token = player1_tokenizer.eos_token\n",
        "\n",
        "player2_model = AutoModelForCausalLM.from_pretrained(\"./trained_player2_model\")\n",
        "player2_tokenizer = AutoTokenizer.from_pretrained(\"cerebras/Cerebras-GPT-111M\")\n",
        "player2_tokenizer.pad_token = player2_tokenizer.eos_token\n",
        "\n",
        "# Función para inicializar un tablero vacío\n",
        "def inicializar_tablero():\n",
        "    return [[\" \" for _ in range(3)] for _ in range(3)]\n",
        "\n",
        "# Función para representar el tablero como texto\n",
        "def representar_tablero(tablero):\n",
        "    return \"\\n\".join([\" | \".join(fila) for fila in tablero])\n",
        "\n",
        "# Función para verificar si hay un ganador\n",
        "def verificar_ganador(tablero):\n",
        "    for fila in tablero:\n",
        "        if fila[0] == fila[1] == fila[2] and fila[0] != \" \":\n",
        "            return fila[0]\n",
        "\n",
        "    for col in range(3):\n",
        "        if tablero[0][col] == tablero[1][col] == tablero[2][col] and tablero[0][col] != \" \":\n",
        "            return tablero[0][col]\n",
        "\n",
        "    if tablero[0][0] == tablero[1][1] == tablero[2][2] and tablero[0][0] != \" \":\n",
        "        return tablero[0][0]\n",
        "\n",
        "    if tablero[0][2] == tablero[1][1] == tablero[2][0] and tablero[0][2] != \" \":\n",
        "        return tablero[0][2]\n",
        "\n",
        "    return None\n",
        "\n",
        "# Función para verificar si el tablero está lleno\n",
        "def tablero_lleno(tablero):\n",
        "    return all(cell != \" \" for fila in tablero for cell in fila)\n",
        "\n",
        "# Función para seleccionar un movimiento válido aleatorio\n",
        "def seleccionar_movimiento_valido(tablero):\n",
        "    movimientos_validos = [\n",
        "        (fila, col) for fila in range(3) for col in range(3) if tablero[fila][col] == \" \"\n",
        "    ]\n",
        "    if movimientos_validos:\n",
        "        fila, col = random.choice(movimientos_validos)\n",
        "        return fila * 3 + col + 1  # Convertir a movimiento en el rango [1-9]\n",
        "    return None\n",
        "\n",
        "# Función para que un jugador haga un movimiento\n",
        "def realizar_jugada(model, tokenizer, tablero, turno, max_reintentos=3):\n",
        "    # Generar el texto del estado actual del juego\n",
        "    tablero_texto = representar_tablero(tablero)\n",
        "    input_text = f\"Tablero:\\n{tablero_texto}\\nTurno: {turno}\\n\"\n",
        "\n",
        "    for _ in range(max_reintentos):\n",
        "        # Tokenizar y generar la jugada\n",
        "        inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128)\n",
        "        output = model.generate(**inputs, max_new_tokens=50, pad_token_id=tokenizer.pad_token_id)\n",
        "        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "        # Extraer el movimiento de la salida generada\n",
        "        try:\n",
        "            movimiento = int(generated_text.split(\"Movimiento: \")[1].strip())\n",
        "            fila, col = divmod(movimiento - 1, 3)\n",
        "            if 1 <= movimiento <= 9 and tablero[fila][col] == \" \":\n",
        "                return movimiento, True  # Movimiento válido\n",
        "        except (IndexError, ValueError):\n",
        "            pass  # Movimiento inválido o error de generación\n",
        "\n",
        "    # Si todos los reintentos fallan, seleccionar un movimiento válido aleatorio\n",
        "    movimiento = seleccionar_movimiento_valido(tablero)\n",
        "    return movimiento, movimiento is not None\n",
        "\n",
        "# Función para que el juez comente sobre el movimiento\n",
        "def comentario_del_juez(tablero, turno, movimiento):\n",
        "    fila, col = divmod(movimiento - 1, 3)\n",
        "    if 1 <= movimiento <= 9 and tablero[fila][col] == \" \":\n",
        "        return f\"COMENTARIO JUEZ: El movimiento {movimiento} por el jugador '{turno}' es válido.\"\n",
        "    else:\n",
        "        return f\"COMENTARIO JUEZ: El movimiento {movimiento} por el jugador '{turno}' es inválido.\"\n",
        "\n",
        "# Función para determinar las posiciones ganadoras\n",
        "def obtener_posiciones_ganadoras(tablero):\n",
        "    for i in range(3):\n",
        "        # Verificar filas\n",
        "        if tablero[i][0] == tablero[i][1] == tablero[i][2] and tablero[i][0] != \" \":\n",
        "            return [(i, 0), (i, 1), (i, 2)]\n",
        "        # Verificar columnas\n",
        "        if tablero[0][i] == tablero[1][i] == tablero[2][i] and tablero[0][i] != \" \":\n",
        "            return [(0, i), (1, i), (2, i)]\n",
        "    # Verificar diagonales\n",
        "    if tablero[0][0] == tablero[1][1] == tablero[2][2] and tablero[0][0] != \" \":\n",
        "        return [(0, 0), (1, 1), (2, 2)]\n",
        "    if tablero[0][2] == tablero[1][1] == tablero[2][0] and tablero[0][2] != \" \":\n",
        "        return [(0, 2), (1, 1), (2, 0)]\n",
        "    return []\n",
        "\n",
        "# Función para mostrar el tablero resaltando las posiciones ganadoras\n",
        "def mostrar_tablero_con_victoria(tablero, posiciones_ganadoras):\n",
        "    tablero_resaltado = [[cell for cell in row] for row in tablero]\n",
        "    for fila, col in posiciones_ganadoras:\n",
        "        tablero_resaltado[fila][col] = f\"*{tablero_resaltado[fila][col]}*\"\n",
        "    return \"\\n\".join([\" | \".join(fila) for fila in tablero_resaltado])\n",
        "\n",
        "# Función para jugar una partida completa\n",
        "def jugar_partida():\n",
        "    tablero = inicializar_tablero()\n",
        "    turno = \"X\"  # Comienza el Jugador 1\n",
        "    jugador_actual = 1\n",
        "\n",
        "    while True:\n",
        "        # Seleccionar el modelo y tokenizador del jugador actual\n",
        "        if jugador_actual == 1:\n",
        "            model = player1_model\n",
        "            tokenizer = player1_tokenizer\n",
        "            modelo_nombre = \"Pythia 160M\"\n",
        "        else:\n",
        "            model = player2_model\n",
        "            tokenizer = player2_tokenizer\n",
        "            modelo_nombre = \"Cerebras-GPT 111M\"\n",
        "\n",
        "        # Realizar la jugada\n",
        "        movimiento, valido = realizar_jugada(model, tokenizer, tablero, turno)\n",
        "\n",
        "        # Comentario del juez\n",
        "        comentario = comentario_del_juez(tablero, turno, movimiento)\n",
        "\n",
        "        if not valido:\n",
        "            print(f\"Movimiento inválido por MODELO {modelo_nombre} (Jugador {jugador_actual}). Turno perdido.\")\n",
        "            print(comentario)\n",
        "            jugador_actual = 3 - jugador_actual  # Cambiar de jugador\n",
        "            turno = \"O\" if turno == \"X\" else \"X\"\n",
        "            continue\n",
        "\n",
        "        # Actualizar el tablero\n",
        "        fila, col = divmod(movimiento - 1, 3)\n",
        "        tablero[fila][col] = turno\n",
        "\n",
        "        # Mostrar el estado actual del tablero\n",
        "        print(f\"\\nMODELO {modelo_nombre} (Jugador {jugador_actual}, {turno}) hizo el movimiento {movimiento}\")\n",
        "        for fila in tablero:\n",
        "            print(fila)\n",
        "        print(comentario)\n",
        "\n",
        "        # Verificar estado del juego\n",
        "        ganador = verificar_ganador(tablero)\n",
        "        if ganador:\n",
        "            posiciones_ganadoras = obtener_posiciones_ganadoras(tablero)\n",
        "            modelo_ganador = \"Pythia 160M\" if ganador == \"X\" else \"Cerebras-GPT 111M\"\n",
        "            print(f\"\\n¡Victoria de MODELO {modelo_ganador} ({ganador})!\")\n",
        "            print(f\"COMENTARIO JUEZ: ¡El jugador '{ganador}' con el modelo {modelo_ganador} ganó!\")\n",
        "            print(\"Posiciones ganadoras:\")\n",
        "            print(mostrar_tablero_con_victoria(tablero, posiciones_ganadoras))\n",
        "            return ganador\n",
        "\n",
        "        if tablero_lleno(tablero):\n",
        "            print(\"\\n¡Empate!\")\n",
        "            return \"Empate\"\n",
        "\n",
        "        # Cambiar de jugador\n",
        "        jugador_actual = 3 - jugador_actual\n",
        "        turno = \"O\" if turno == \"X\" else \"X\"\n",
        "\n",
        "# Jugar una partida completa\n",
        "resultado = jugar_partida()\n",
        "print(f\"Resultado final: {resultado}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
